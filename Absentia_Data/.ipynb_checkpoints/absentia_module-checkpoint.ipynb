{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so you can imagine that the Custom Scaler is build on it\n",
    "\n",
    "\n",
    "\n",
    "# create the Custom Scaler class\n",
    "\n",
    "class CustomScaler(BaseEstimator,TransformerMixin): \n",
    "    \n",
    "    # init or what information we need to declare a CustomScaler object\n",
    "    # and what is calculated/declared as we do\n",
    "    \n",
    "    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n",
    "        \n",
    "        # scaler is nothing but a Standard Scaler object\n",
    "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
    "        # with some columns 'twist'\n",
    "        self.columns = columns\n",
    "        self.mean_ = None\n",
    "        self.var_ = None\n",
    "        \n",
    "    \n",
    "    # the fit method, which, again based on StandardScale\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        self.mean_ = np.mean(X[self.columns])\n",
    "        self.var_ = np.var(X[self.columns])\n",
    "        return self\n",
    "    \n",
    "    # the transform method which does the actual scaling\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        \n",
    "        # record the initial order of the columns\n",
    "        init_col_order = X.columns\n",
    "        \n",
    "        # scale all features that you chose when creating the instance of the class\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
    "        \n",
    "        # declare a variable containing all information that was not scaled\n",
    "        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n",
    "        \n",
    "        # return a data frame which contains all scaled features and all 'not scaled' features\n",
    "        # use the original order (that you recorded in the beginning)\n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abesntial Main Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class absentia_model():\n",
    "    def __init__(self):\n",
    "        with open('model.pkl','rb') as model,open('scaler.pkl','rb') as scalar:\n",
    "            self.model = pickle.load(model)\n",
    "            self.scalar = pickle.load(scalar)\n",
    "        self.data = None\n",
    "    \n",
    "    \n",
    "    def load_and_clean_data(self,datafile):\n",
    "        \n",
    "        df = pd.read_csv(datafile)\n",
    "        data = df.copy()\n",
    "        data = data.drop(['ID'],axis =1 )\n",
    "        \n",
    "        \n",
    "        \n",
    "        reason_dummies = pd.get_dummies(data['Reason for Absence'],drop_first=True)\n",
    "        reason_1 = reason_dummies.iloc[:,0:14].max(axis=1)\n",
    "        reason_2 = reason_dummies.iloc[:,14:17].max(axis=1)\n",
    "        reason_3 = reason_dummies.iloc[:,17:20].max(axis=1)\n",
    "        reason_4 = reason_dummies.iloc[:,20:28].max(axis=1)\n",
    "        data = data.drop(['Reason for Absence'],axis=1)\n",
    "        \n",
    "        encoded_data = pd.concat([data,reason_1,reason_2,reason_3,reason_4],axis=1)\n",
    "        new_columns = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',  \\\n",
    "       'Daily Work Load Average', 'Body Mass Index', 'Education',                    \\\n",
    "       'Children', 'Pets', 'Absenteeism Time in Hours', 'Reason_1',  'Reason_2',  'Reason_3',  'Reason_4'] \n",
    "        \n",
    "        encoded_data.columns = new_columns\n",
    "        rearange_data = encoded_data[['Absenteeism Time in Hours','Date', 'Transportation Expense', 'Distance to Work', 'Age',\n",
    "       'Daily Work Load Average', 'Body Mass Index', 'Education',\n",
    "       'Children', 'Pets',  'Reason_1',  'Reason_2',  'Reason_3',  'Reason_4']]\n",
    "        \n",
    "        month = []\n",
    "        day = []\n",
    "        for i in range(rearange_data.shape[0]):\n",
    "            day.append(rearange_data['Date'][i].split('/')[0])\n",
    "            month.append(rearange_data['Date'][i].split('/')[1])\n",
    "        \n",
    "        rearange_data['Day'] = day\n",
    "        rearange_data['Month'] = month\n",
    "        rearange_data['Day'] = rearange_data['Day'].astype(int)\n",
    "        rearange_data['Month'] = rearange_data['Month'].astype(int)\n",
    "        \n",
    "        rearange_data  = rearange_data.drop(['Date','Absenteeism Time in Hours'],axis=1)\n",
    "        \n",
    "        all_cols = rearange_data.columns.values\n",
    "        \n",
    "        cols_to_remove = ['Day','Month','Daily Work Load Average','Education']\n",
    "        \n",
    "        to_include_col = [ i for i in all_cols if i not in cols_to_remove]\n",
    "        \n",
    "        self.preprocesseddata = rearange_data[to_include_col].copy()\n",
    "        \n",
    "        scaled_data = self.scalar.transform(rearange_data[to_include_col])\n",
    "        self.data = scaled_data\n",
    "        \n",
    "    def predicted_probability(self):\n",
    "        \n",
    "        return self.model.predict_proba(self.data)[:,1]\n",
    "    \n",
    "    def predicted_output_category(self):\n",
    "        return self.model.predict(self.data)\n",
    "    \n",
    "    def predicted_output(self):\n",
    "        self.preprocesseddata['Probability'] = self.model.predict_proba(self.data)[:,1]\n",
    "        self.preprocesseddata['Prediction'] = self.model.predict(self.data)\n",
    "        return self.preprocesseddata\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowenv]",
   "language": "python",
   "name": "conda-env-tensorflowenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
